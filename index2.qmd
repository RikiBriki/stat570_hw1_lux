# Project start

In this chapter, we are going to work together on a very simple project. This project will stay with us until the end of the book. As we will go deeper into the book together, you will rewrite that project by implementing the techniques I will teach you. By the end of the book you will have built a reproducible analytical pipeline. To get things going, we are going to keep it simple; our goal here is to get an analysis done, that's it. We won't focus on reproducibility. We are going to download some data, and analyse it, that's it.

## Housing in Luxembourg

We are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. Did you know that Luxembourg is a constitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and actually the last Grand-Duchy in the World? Also, what you should know to understand what we will be doing is that the country of Luxembourg is divided into Cantons, and each Cantons into Communes. If Luxembourg was the USA, Cantons would be States and Communes would be Counties (or Parishes or Boroughs). What's confusing is that "Luxembourg" is also the name of a Canton, and of a Commune, which also has the status of a city and is the capital of the country. So Luxembourg the country, is divided into Cantons, one of which is called Luxembourg as well, cantons are divided into communes, and inside the canton of Luxembourg, there's the commune of Luxembourg which is also the city of Luxembourg, sometimes called Luxembourg City, which is the capital of the country.

::: {.content-hidden when-format="pdf"}
<figure>

![Luxembourg is about as big as the US State of Rhode Island.](images/lux_rhode_island.png){alt="Luxembourg is about as big as the US State of Rhode Island."}</img>

<figcaption>Luxembourg is about as big as the US State of Rhode Island.</figcaption>

</figure>
:::

::: {.content-visible when-format="pdf"}
```{r, echo = F, out.width="300px"}
#| fig-cap: "Luxembourg is about as big as the US State of Rhode Island."
knitr::include_graphics("images/lux_rhode_island.png")
```
:::

What you should also know is that the population is about 645,000 as of writing (January 2023), half of which are foreigners. Around 400,000 persons work in Luxembourg, of which half do not live in Luxembourg; so every morning from Monday to Friday, 200,000 people enter the country to work and then leave in the evening to go back to either Belgium, France or Germany, the neighbouring countries. As you can imagine, this puts enormous pressure on the transportation system and on the roads, but also on the housing market; everyone wants to live in Luxembourg to avoid the horrible daily commute, and everyone wants to live either in the capital city, or in the second largest urban area in the south, in a city called Esch-sur-Alzette.

The plot below shows the value of the House Price Index over time for Luxembourg and the European Union:

```{r, echo = F}
#https://ec.europa.eu/eurostat/databrowser/bookmark/21530d9a-c423-4ab7-998a-7170326136cd?lang=en 
housing_lu_eu <- read.csv("datasets/prc_hpi_a__custom_4705395_page_linear.csv.gz")  
withr::with_package("ggplot2",   
      {     ggplot(data = housing_lu_eu) + 
          geom_line(aes(y = OBS_VALUE, x = TIME_PERIOD, group = geo, colour = geo),                 linewidth = 1.5) +       labs(title = "House price and sales index (2010 = 100)",            caption = "Source: Eurostat") +       theme_minimal() +       theme(legend.position = "bottom")   }   ) 
```

If you want to download the data, click [here](https://github.com/b-rodrigues/rap4all/raw/master/datasets/prc_hpi_a__custom_4705395_page_linear.csv.gz)[^index2-1].

[^index2-1]: https://is.gd/AET0ir

Let us paste the definition of the HPI in here (taken from the HPI's [metadata](https://archive.is/OrQwA)[^index2-2] page):

[^index2-2]: https://archive.is/OrQwA, archived link for posterity.

*The House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.*

So from the plot, we can see that the price of dwellings more than doubled between 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg, and 138.92 for the European Union as a whole.

There is a lot of heterogeneity though; the capital and the communes right next to the capital are much more expensive than communes from the less densely populated north, for example. The south of the country is also more expensive than the north, but not as much as the capital and surrounding communes. Not only is price driven by demand, but also by scarcity; in 2021, 0.5% of residents owned 50% of the buildable land for housing purposes (Source: *Observatoire de l'Habitat, Note 29*, [archived download link](https://archive.org/download/note-29/note-29.pdf)[^index2-3]).

[^index2-3]: https://archive.org/download/note-29/note-29.pdf

Our project will be quite simple; we are going to download some data, supplied as an Excel file, compiled by the Housing Observatory (*Observatoire de l'Habitat*, a service from the Ministry of Housing, which monitors the evolution of prices in the housing market, among other useful services like the identification of vacant lots). The advantage of their data when compared to Eurostat's data is that the data is disaggregated by commune. The disadvantage is that they only supply nominal prices, and no index (and the data is trapped inside Excel and not ready for analysis with R). Nominal prices are the prices that you read on price tags in shops. The problem with nominal prices is that it is difficult to compare them through time. Ask yourself the following question: would you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably would have preferred them in 2003, as you could purchase a lot more with \$500 then than now. In fact, according to a random inflation calculator I googled, to match the purchasing power of \$500 in 2003, you'd need to have \$793 in 2023 (and I'd say that we find very similar values for €). But it doesn't really matter if that calculation is 100% correct: what matters is that the value of money changes, and comparisons through time are difficult, hence why an index is quite useful. So we are going to convert these nominal prices to real prices. Real prices take inflation into account and so allow us to compare prices through time.

So to summarise; our goal is to:

-   Get data trapped inside an Excel file into a neat data frame;
-   Convert nominal to real prices using a simple method;
-   Make some tables and plots and call it a day (for now).

We are going to start in the most basic way possible; we are simply going to write a script and deal with each step separately.

::: {.content-visible when-format="pdf"}
\newpage
:::

## Saving trapped data from Excel

Getting data from Excel into a tidy data frame can be very tricky. This is because very often, Excel is used as some kind of dashboard or presentation tool. So data is made human-readable, in contrast to machine-readable. Let us quickly discuss this topic as it is essential to grasp the difference between the two (and in our experience, a lot of collective pain inflicted to statisticians and researchers could have been avoided if this concept was more well-known). The picture below shows an Excel file made for human consumption:

::: {.content-hidden when-format="pdf"}
<figure>

![An Excel file meant for human eyes.](images/obs_hab_xlsx_overview.png){alt="An Excel file meant for human eyes."}</img>

<figcaption>An Excel file meant for human eyes.</figcaption>

</figure>
:::

::: {.content-visible when-format="pdf"}
```{r, echo = F}
#| fig-cap: "An Excel file meant for human eyes."
knitr::include_graphics("images/obs_hab_xlsx_overview.png")
```
:::

So why is this file not machine-readable? Here are some issues:

-   The table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;
-   The spreadsheet starts with a header that contains an image and some text;
-   Numbers are text and use "," as the thousands separator;
-   You don't see it in the screenshot, but each year is in a separate sheet.

That being said, this Excel file is still very tame, and going from this Excel to a tidy data frame will not be too difficult. In fact, we suspect that whoever made this Excel file is well aware of the contradicting requirements of human and machine-readable formatting of data, and strove to find a compromise. Because more often than not, getting human-readable data into a machine-readable format is a nightmare. We could call data like this *machine-friendly* data.

If you want to follow along, you can download the Excel file [here](https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx)[^index2-4] (downloaded on January 2023 from the [luxembourguish open data portal](https://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/)[^index2-5]). But you don't need to follow along with code, because I will link the completed scripts for you to download later.

[^index2-4]: https://is.gd/1vvBAc

[^index2-5]: https://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/

Each sheet contains a dataset with the following columns:

-   *Commune*: the commune (the smallest administrative division of territory);
-   *Nombre d'offres*: the total number of selling offers;
-   *Prix moyen annoncé en Euros courants*: Average selling price in nominal Euros;
-   *Prix moyen annoncé au m2 en Euros courants*: Average selling price in square meters in nominal Euros.

For ease of presentation, I'm going to show you each step of the analysis here separately, but I'll be putting everything together in a single script once I'm done explaining each step. So first, let's load some packages:

```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(purrr)
library(readxl)
library(stringr)
library(janitor)
```

Even though this book is not about analysing data per se, let me just briefly explain what these packages do, in case you're not familiar with them. The `{dplyr}` package provides many functions for data manipulation, for example aggregating group-wise. `{purrr}` is a package for functional programming, a programming paradigm that I'll introduce later in the book, `{readxl}` reads in Excel workbooks, `{stringr}` is a package for manipulating strings, and finally `{janitor}` [@firke2023] provides some very nice functions, to perform some common tasks like easily rename every column of a data frame in snake case.

Next, the code below downloads the data, and puts it in a data frame:

```{r, warning = FALSE, message = FALSE}
# The url below points to an Excel file 
# hosted on the book’s github repository 
url <- "https://is.gd/1vvBAc"  
raw_data <- tempfile(fileext = ".xlsx")  
download.file(url, raw_data, method = "auto", mode = "wb")  
sheets <- excel_sheets(raw_data)  
read_clean <- function(..., sheet){  
  read_excel(..., sheet = sheet) |>   
    mutate(year = sheet) }  
raw_data <- map(   
  sheets,   
  ~read_clean(raw_data,           
              skip = 10,       
              sheet = .)   
  ) |>   
  bind_rows() |>   
  clean_names()  
raw_data <- raw_data |> 
  rename(     
    locality = commune,  
    n_offers = nombre_doffres,  
    average_price_nominal_euros = prix_moyen_annonce_en_courant, 
    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant, 
    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant  
    ) |>  
  mutate(locality = str_trim(locality)) |>   
  select(year, locality, n_offers, starts_with("average"))
```

If you are familiar with the `{tidyverse}` [@wickham2019b] the above code should be quite easy to follow. We start by downloading the raw Excel file and saving the sheet names into a variable. We then use a function called `read_clean()`, which takes the path to the Excel file and the sheet names as an argument to read the required sheet into a data frame. We use `skip = 10` to skip the first 10 lines in each Excel sheet because the first 10 lines contain a header. The last thing this function does is add a new column called `year` which contains the year of the data. We're lucky because the sheet names are the years: "2010", "2011" and so on. We then map this function to the list of sheet names, thus reading in all the data from all the sheets into one list of data frames. We then use `bind_rows()`, to bind each data frame into a single data frame, by row. Finally, we rename the columns (by translating their names from French to English) and only select the required columns. If you don't understand each step of what is going on, don't worry too much about it; this book is not about learning how to use R.

Running this code results in a neat data set:

`{r} raw_data}`

But there's a problem: columns that should be of type numeric are of type character instead (`average_price_nominal_euros` and `average_price_m2_nominal_euros`). There's also another issue, which you would eventually catch as you'll explore the data: the naming of the communes is not consistent. Let's take a look:

`{r} raw_data |>}   filter(grepl("Luxembourg", locality)) |>    count(locality)`

We can see that the city of Luxembourg is spelled in two different ways. It's the same with another commune, Pétange:

`{r} raw_data |>}   filter(grepl("P.tange", locality)) |>     count(locality)`

So sometimes it is spelled correctly, with an "é", sometimes not. Let's write some code to correct both these issues:

`{r} raw_data <- raw_data |>}   mutate(     locality = ifelse(grepl("Luxembourg-Ville", locality),                       "Luxembourg",                       locality),          locality = ifelse(grepl("P.tange", locality),                            "Pétange",                            locality)          ) |>     mutate(across(starts_with("average"),          as.numeric))`

Now this is interesting -- converting the `average` columns to numeric resulted in some `NA` values. Let's see what happened:

`{r} raw_data |>}   filter(is.na(average_price_nominal_euros))`

It turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let's go back to the raw data to see what this is about:

::: {.content-hidden when-format="pdf"}
<figure>

![Always look at your data.](images/obs_hab_xlsx_missing.png){alt="Always look at your data."}</img>

<figcaption>Always look at your data.</figcaption>

</figure>
:::

::: {.content-visible when-format="pdf"}
```{r, echo = F}
#| fig-cap: "Always look at your data." 
knitr::include_graphics("images/obs_hab_xlsx_missing.png")
```
:::

So it turns out that there are some rows that we need to remove. We can start by removing rows where `locality` is missing. Then we have a row where `locality` is equal to "Total d'offres". This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. The very last row states the source of the data and we can also remove it. Finally, in the screenshot above, we see another row that we don't see in our filtered data frame: one where `n_offers` is missing. This row gives the national average for columns

```{r}
raw_data$average_price_m2_nominal_euros[raw_data$average_price_m2_nominal_euros=="*"] = NA
```

```{r}
raw_data_ikibinooon = raw_data[raw_data$year=='2010',]
raw_data_ikibinooon[order(-raw_data_ikibinooon$n_offers),]
```
