{
  "hash": "8f9798dffff10d6ef6229614fad89604",
  "result": {
    "markdown": "# Project start\n\nIn this chapter, we are going to work together on a very simple project. This\nproject will stay with us until the end of the book. As we will go deeper into\nthe book together, you will rewrite that project by implementing the techniques\nI will teach you. By the end of the book you will have built a reproducible\nanalytical pipeline. To get things going, we are going to keep it simple; our\ngoal here is to get an analysis done, that's it. We won't focus on\nreproducibility. We are going to download some data, and analyse it, that's it.\n\n## Housing in Luxembourg\n\nWe are going to download data about house prices in Luxembourg. Luxembourg is a\nlittle Western European country the author hails from that looks like a shoe and\nis about the size of .98 Rhode Islands. Did you know that Luxembourg is a\nconstitutional monarchy, and not a kingdom like Belgium, but a Grand-Duchy, and\nactually the last Grand-Duchy in the World? Also, what you should know to\nunderstand what we will be doing is that the country of Luxembourg is divided\ninto Cantons, and each Cantons into Communes. If Luxembourg was the USA, Cantons\nwould be States and Communes would be Counties (or Parishes or Boroughs). What’s\nconfusing is that “Luxembourg” is also the name of a Canton, and of a Commune,\nwhich also has the status of a city and is the capital of the country. So\nLuxembourg the country, is divided into Cantons, one of which is called\nLuxembourg as well, cantons are divided into communes, and inside the canton of\nLuxembourg, there's the commune of Luxembourg which is also the city of\nLuxembourg, sometimes called Luxembourg City, which is the capital of the\ncountry.\n\n::: {.content-hidden when-format=\"pdf\"}\n<figure>\n    <img src=\"images/lux_rhode_island.png\"\n         alt=\"Luxembourg is about as big as the US State of Rhode Island.\"></img>\n    <figcaption>Luxembourg is about as big as the US State of Rhode Island.</figcaption>\n</figure>\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![Luxembourg is about as big as the US State of Rhode Island.](images/lux_rhode_island.png){width=300px}\n:::\n:::\n\n:::\n\nWhat you should also know is that the population is about 645,000 as of writing\n(January 2023), half of which are foreigners. Around 400,000 persons work in\nLuxembourg, of which half do not live in Luxembourg; so every morning from\nMonday to Friday, 200,000 people enter the country to work and then leave in the\nevening to go back to either Belgium, France or Germany, the neighbouring\ncountries. As you can imagine, this puts enormous pressure on the transportation\nsystem and on the roads, but also on the housing market; everyone wants to live\nin Luxembourg to avoid the horrible daily commute, and everyone wants to live\neither in the capital city, or in the second largest urban area in the south, in\na city called Esch-sur-Alzette.\n\nThe plot below shows the value of the House Price Index over time for Luxembourg\nand the European Union:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output-display}\n![](project_start_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf you want to download the data, click\n[here](https://github.com/b-rodrigues/rap4all/raw/master/datasets/prc_hpi_a__custom_4705395_page_linear.csv.gz)^[https://is.gd/AET0ir].\n\nLet us paste the definition of the HPI in here (taken from the HPI's\n[metadata](https://archive.is/OrQwA)^[https://archive.is/OrQwA, archived link for posterity.] page):\n\n*The House Price Index (HPI) measures inflation in the residential property market. The HPI\ncaptures price changes of all types of dwellings purchased by households (flats, detached houses,\nterraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are\nexcluded. The land component of the dwelling is included.*\n\nSo from the plot, we can see that the price of dwellings more than doubled\nbetween 2010 and 2021; the value of the index is 214.81 in 2021 for Luxembourg,\nand 138.92 for the European Union as a whole.\n\nThere is a lot of heterogeneity though; the capital and the communes right next\nto the capital are much more expensive than communes from the less densely\npopulated north, for example. The south of the country is also more expensive\nthan the north, but not as much as the capital and surrounding communes. Not\nonly is price driven by demand, but also by scarcity; in 2021, 0.5% of residents\nowned 50% of the buildable land for housing purposes (Source: *Observatoire de\nl'Habitat, Note 29*, [archived download\nlink](https://archive.org/download/note-29/note-29.pdf)^[https://archive.org/download/note-29/note-29.pdf]).\n\nOur project will be quite simple; we are going to download some data, supplied\nas an Excel file, compiled by the Housing Observatory (*Observatoire de\nl'Habitat*, a service from the Ministry of Housing, which monitors the evolution\nof prices in the housing market, among other useful services like the\nidentification of vacant lots). The advantage of their data when compared to\nEurostat's data is that the data is disaggregated by commune. The disadvantage\nis that they only supply nominal prices, and no index (and the data is trapped\ninside Excel and not ready for analysis with R). Nominal prices are the prices\nthat you read on price tags in shops. The problem with nominal prices is that it\nis difficult to compare them through time. Ask yourself the following question:\nwould you prefer to have had 500€ (or USDs) in 2003 or in 2023? You probably\nwould have preferred them in 2003, as you could purchase a lot more with $500\nthen than now. In fact, according to a random inflation calculator I googled, to\nmatch the purchasing power of $500 in 2003, you'd need to have $793 in 2023 (and\nI'd say that we find very similar values for €). But it doesn't really matter if\nthat calculation is 100% correct: what matters is that the value of money\nchanges, and comparisons through time are difficult, hence why an index is quite\nuseful. So we are going to convert these nominal prices to real prices. Real\nprices take inflation into account and so allow us to compare prices through\ntime.\n\nSo to summarise; our goal is to:\n\n- Get data trapped inside an Excel file into a neat data frame;\n- Convert nominal to real prices using a simple method;\n- Make some tables and plots and call it a day (for now).\n\nWe are going to start in the most basic way possible; we are simply going to\nwrite a script and deal with each step separately.\n\n::: {.content-visible when-format=\"pdf\"}\n\\newpage\n:::\n\n## Saving trapped data from Excel\n\nGetting data from Excel into a tidy data frame can be very tricky. This is\nbecause very often, Excel is used as some kind of dashboard or presentation\ntool. So data is made human-readable, in contrast to machine-readable. Let us\nquickly discuss this topic as it is essential to grasp the difference between\nthe two (and in our experience, a lot of collective pain inflicted to\nstatisticians and researchers could have been avoided if this concept was more\nwell-known). The picture below shows an Excel file made for human consumption:\n\n::: {.content-hidden when-format=\"pdf\"}\n<figure>\n    <img src=\"images/obs_hab_xlsx_overview.png\"\n         alt=\"An Excel file meant for human eyes.\"></img>\n    <figcaption>An Excel file meant for human eyes.</figcaption>\n</figure>\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![An Excel file meant for human eyes.](images/obs_hab_xlsx_overview.png){width=500}\n:::\n:::\n\n:::\n\nSo why is this file not machine-readable? Here are some issues:\n\n- The table does not start in the top-left corner of the spreadsheet, which is where most importing tools expect it to be;\n- The spreadsheet starts with a header that contains an image and some text;\n- Numbers are text and use \",\" as the thousands separator;\n- You don't see it in the screenshot, but each year is in a separate sheet.\n\nThat being said, this Excel file is still very tame, and going from this Excel\nto a tidy data frame will not be too difficult. In fact, we suspect that whoever\nmade this Excel file is well aware of the contradicting requirements of human\nand machine-readable formatting of data, and strove to find a compromise.\nBecause more often than not, getting human-readable data into a machine-readable\nformat is a nightmare. We could call data like this *machine-friendly* data.\n\nIf you want to follow along, you can download the Excel file\n[here](https://github.com/b-rodrigues/rap4all/raw/master/datasets/vente-maison-2010-2021.xlsx)^[https://is.gd/1vvBAc]\n(downloaded on January 2023 from the [luxembourguish open data\nportal](https://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/)^[https://data.public.lu/en/datasets/prix-annonces-des-logements-par-commune/]).\nBut you don’t need to follow along with code, because I will link the completed\nscripts for you to download later.\n\nEach sheet contains a dataset with the following columns:\n\n- *Commune*: the commune (the smallest administrative division of territory);\n- *Nombre d'offres*: the total number of selling offers;\n- *Prix moyen annoncé en Euros courants*: Average selling price in nominal Euros;\n- *Prix moyen annoncé au m2 en Euros courants*: Average selling price in square meters in nominal Euros.\n\nFor ease of presentation, I'm going to show you each step of the analysis here\nseparately, but I'll be putting everything together in a single script once\nI'm done explaining each step. So first, let's load some packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(readxl)\nlibrary(stringr)\nlibrary(janitor)\n```\n:::\n\n\nEven though this book is not about analysing data per se, let me just briefly\nexplain what these packages do, in case you're not familiar with them. The\n`{dplyr}` package provides many functions for data manipulation, for example\naggregating group-wise. `{purrr}` is a package for functional programming, a\nprogramming paradigm that I'll introduce later in the book, `{readxl}` reads in\nExcel workbooks, `{stringr}` is a package for manipulating strings, and finally\n`{janitor}` [@firke2023] provides some very nice functions, to perform some\ncommon tasks like easily rename every column of a data frame in snake case.\n\nNext, the code below downloads the data, and puts it in a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The url below points to an Excel file\n# hosted on the book’s github repository\nurl <- \"https://is.gd/1vvBAc\"\n\nraw_data <- tempfile(fileext = \".xlsx\")\n\ndownload.file(url, raw_data,\n              method = \"auto\",\n              mode = \"wb\")\n\nsheets <- excel_sheets(raw_data)\n\nread_clean <- function(..., sheet){\n  read_excel(..., sheet = sheet) |>\n    mutate(year = sheet)\n}\n\nraw_data <- map(\n  sheets,\n  ~read_clean(raw_data,\n              skip = 10,\n              sheet = .)\n                   ) |>\n  bind_rows() |>\n  clean_names()\n\nraw_data <- raw_data |>\n  rename(\n    locality = commune,\n    n_offers = nombre_doffres,\n    average_price_nominal_euros = prix_moyen_annonce_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,\n    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant\n  ) |>\n  mutate(locality = str_trim(locality)) |>\n  select(year, locality, n_offers, starts_with(\"average\"))\n```\n:::\n\n\nIf you are familiar with the `{tidyverse}` [@wickham2019b] the above code should\nbe quite easy to follow. We start by downloading the raw Excel file and saving\nthe sheet names into a variable. We then use a function called `read_clean()`,\nwhich takes the path to the Excel file and the sheet names as an argument to\nread the required sheet into a data frame. We use `skip = 10` to skip the first\n10 lines in each Excel sheet because the first 10 lines contain a header. The\nlast thing this function does is add a new column called `year` which contains\nthe year of the data. We’re lucky because the sheet names are the years: \"2010\",\n\"2011\" and so on. We then map this function to the list of sheet names, thus\nreading in all the data from all the sheets into one list of data frames. We\nthen use `bind_rows()`, to bind each data frame into a single data frame, by\nrow. Finally, we rename the columns (by translating their names from French to\nEnglish) and only select the required columns. If you don’t understand each step\nof what is going on, don’t worry too much about it; this book is not about\nlearning how to use R.\n\nRunning this code results in a neat data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,343 × 5\n   year  locality    n_offers average_price_nominal_euros average_price_m2_nom…¹\n   <chr> <chr>          <dbl> <chr>                       <chr>                 \n 1 2010  Bascharage       192 593698.31000000006          3603.57               \n 2 2010  Beaufort         266 461160.29                   2902.76               \n 3 2010  Bech              65 621760.22                   3280.51               \n 4 2010  Beckerich        176 444498.68                   2867.88               \n 5 2010  Berdorf          111 504040.85                   3055.99               \n 6 2010  Bertrange        264 795338.87                   4266.46               \n 7 2010  Bettembourg      304 555628.29                   3343.22               \n 8 2010  Bettendorf        94 495074.38                   3235.26               \n 9 2010  Betzdorf         119 625914.47                   3343.05               \n10 2010  Bissen            70 516465.57                   3321.65               \n# ℹ 1,333 more rows\n# ℹ abbreviated name: ¹​average_price_m2_nominal_euros\n```\n:::\n:::\n\n\nBut there’s a problem: columns that should be of type numeric are of type\ncharacter instead (`average_price_nominal_euros` and\n`average_price_m2_nominal_euros`). There's also another issue, which you would\neventually catch as you’ll explore the data: the naming of the communes is not\nconsistent. Let's take a look:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data |>\n  filter(grepl(\"Luxembourg\", locality)) |>\n  count(locality)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  locality             n\n  <chr>            <int>\n1 Luxembourg           9\n2 Luxembourg-Ville     2\n```\n:::\n:::\n\n\nWe can see that the city of Luxembourg is spelled in two different ways. It's\nthe same with another commune, Pétange:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data |>\n  filter(grepl(\"P.tange\", locality)) |>\n  count(locality)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  locality     n\n  <chr>    <int>\n1 Petange      9\n2 Pétange      2\n```\n:::\n:::\n\n\nSo sometimes it is spelled correctly, with an \"é\", sometimes not. Let's write\nsome code to correct both these issues:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data <- raw_data |>\n  mutate(\n    locality = ifelse(grepl(\"Luxembourg-Ville\", locality),\n                      \"Luxembourg\",\n                      locality),\n         locality = ifelse(grepl(\"P.tange\", locality),\n                           \"Pétange\",\n                           locality)\n         ) |>\n  mutate(across(starts_with(\"average\"),\n         as.numeric))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `across(starts_with(\"average\"), as.numeric)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n```\n:::\n:::\n\n\nNow this is interesting -- converting the `average` columns to numeric resulted\nin some `NA` values. Let's see what happened:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data |>\n  filter(is.na(average_price_nominal_euros))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 290 × 5\n   year  locality         n_offers average_price_nomina…¹ average_price_m2_nom…²\n   <chr> <chr>               <dbl>                  <dbl>                  <dbl>\n 1 2010  Consthum               29                     NA                     NA\n 2 2010  Esch-sur-Sûre           7                     NA                     NA\n 3 2010  Heiderscheid           29                     NA                     NA\n 4 2010  Hoscheid               26                     NA                     NA\n 5 2010  Saeul                  14                     NA                     NA\n 6 2010  <NA>                   NA                     NA                     NA\n 7 2010  <NA>                   NA                     NA                     NA\n 8 2010  Total d'offres      19278                     NA                     NA\n 9 2010  <NA>                   NA                     NA                     NA\n10 2010  Source : Minist…       NA                     NA                     NA\n# ℹ 280 more rows\n# ℹ abbreviated names: ¹​average_price_nominal_euros,\n#   ²​average_price_m2_nominal_euros\n```\n:::\n:::\n\n\nIt turns out that there are no prices for certain communes, but that we also\nhave some rows with garbage in there. Let's go back to the raw data to see what\nthis is about:\n\n::: {.content-hidden when-format=\"pdf\"}\n<figure>\n    <img src=\"images/obs_hab_xlsx_missing.png\"\n         alt=\"Always look at your data.\"></img>\n    <figcaption>Always look at your data.</figcaption>\n</figure>\n:::\n\n::: {.content-visible when-format=\"pdf\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![Always look at your data.](images/obs_hab_xlsx_missing.png){width=570}\n:::\n:::\n\n:::\n\nSo it turns out that there are some rows that we need to remove. We can start by\nremoving rows where `locality` is missing. Then we have a row where `locality`\nis equal to \"Total d'offres\". This is simply the total of every offer from every\ncommune. We could keep that in a separate data frame, or even remove it. The\nvery last row states the source of the data and we can also remove it. Finally,\nin the screenshot above, we see another row that we don't see in our filtered\ndata frame: one where `n_offers` is missing. This row gives the national average\nfor columns `average_prince_nominal_euros` and `average_price_m2_nominal_euros`.\nWhat we are going to do is create two datasets: one with data on communes, and\nthe other on national prices. Let's first remove the rows stating the sources:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nraw_data <- raw_data |>\n  filter(!grepl(\"Source\", locality))\n```\n:::\n\n\nLet's now only keep the communes in our data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommune_level_data <- raw_data |>\n    filter(!grepl(\"nationale|offres\", locality),\n           !is.na(locality))\n```\n:::\n\n\nAnd let's create a dataset with the national data as well:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncountry_level <- raw_data |>\n  filter(grepl(\"nationale\", locality)) |>\n  select(-n_offers)\n\noffers_country <- raw_data |>\n  filter(grepl(\"Total d.offres\", locality)) |>\n  select(year, n_offers)\n\ncountry_level_data <- full_join(country_level, offers_country) |>\n  select(year, locality, n_offers, everything()) |>\n  mutate(locality = \"Grand-Duchy of Luxembourg\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(year)`\n```\n:::\n:::\n\n\nNow the data looks clean, and we can start the actual analysis... or can we?\nBefore proceeding, it would be nice to make sure that we got every commune in\nthere. For this, we need a list of communes from Luxembourg. [Thankfully,\nWikipedia has such a\nlist](https://en.wikipedia.org/wiki/List_of_communes_of_Luxembourg)^[https://w.wiki/6nPu].\n\nAn issue with scraping tables off the web is that they might change in the\nfuture. It is therefore a good idea to save the page by right clicking on it and\nthen selecting save as, and then re-hosting it. I use Github pages to re-host\nthe Wikipedia page above\n[here](https://b-rodrigues.github.io/list_communes/)^[https://is.gd/lux_communes].\nI now have full control of this page, and won't get any bad surprises if someone\ndecides to eventually update it. Instead of re-hosting it, you could simply save\nit as any other file of your project.\n\nSo let's scrape and save this list:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncurrent_communes <- \"https://is.gd/lux_communes\" |>\n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(2) |>\n  janitor::clean_names() |>\n  dplyr::filter(name_2 != \"Name\") |>\n  dplyr::rename(commune = name_2) |>\n  dplyr::mutate(commune = stringr::str_remove(commune, \" .$\"))\n```\n:::\n\n\nWe scrape the table from the re-hosted Wikipedia page using `{rvest}`.\n`rvest::html_table()` returns a list of tables from the Wikipedia table, and\nthen we use `purrr::pluck()` to keep the second table from the website, which is\nwhat we need (I made the calls to the packages explicit, because you might not\nbe familiar with these packages). `janitor::clean_names()` transforms column\nnames written for human eyes into machine-friendly names (for example `Growth\nrate in %` would be transformed to `growth_rate_in_percent`) and then I use\nthe `{dplyr}` package for some further cleaning and renaming; the very last\nstep removes a dagger symbol next to certain communes names, in other words\nit turns \"Commune †\" into \"Commune\".\n\nLet’s see if we have all the communes in our data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetdiff(unique(commune_level_data$locality),\n        current_communes$commune)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Bascharage\"          \"Boevange-sur-Attert\" \"Burmerange\"         \n [4] \"Clémency\"            \"Consthum\"            \"Ermsdorf\"           \n [7] \"Erpeldange\"          \"Eschweiler\"          \"Heiderscheid\"       \n[10] \"Heinerscheid\"        \"Hobscheid\"           \"Hoscheid\"           \n[13] \"Hosingen\"            \"Luxembourg\"          \"Medernach\"          \n[16] \"Mompach\"             \"Munshausen\"          \"Neunhausen\"         \n[19] \"Rosport\"             \"Septfontaines\"       \"Tuntange\"           \n[22] \"Wellenstein\"         \"Kaerjeng\"           \n```\n:::\n:::\n\n\nWe see many communes that are in our `commune_level_data`, but not in\n`current_communes`. There’s one obvious reason: differences in spelling, for\nexample, \"Kaerjeng\" in our data, but \"Käerjeng\" in the table from Wikipedia. But\nthere’s also a less obvious reason; since 2010, several communes have merged\ninto new ones. So there are communes that are in our data in 2010 and\n2011, but disappear from 2012 onwards. So we need to do several things: first,\nget a list of all existing communes from 2010 onwards, and then, harmonise\nspelling. Here again, we can use a list from Wikipedia, and here again, I decide\nto re-host it on Github pages to avoid problems in the future:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformer_communes <- \"https://is.gd/lux_former_communes\" |>\n  rvest::read_html() |>\n  rvest::html_table() |>\n  purrr::pluck(3) |>\n  janitor::clean_names() |>\n  dplyr::filter(year_dissolved > 2009)\n\nformer_communes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 3\n   name                year_dissolved reason                         \n   <chr>                        <int> <chr>                          \n 1 Bascharage                    2011 merged to form Käerjeng        \n 2 Boevange-sur-Attert           2018 merged to form Helperknapp     \n 3 Burmerange                    2011 merged into Schengen           \n 4 Clemency                      2011 merged to form Käerjeng        \n 5 Consthum                      2011 merged to form Parc Hosingen   \n 6 Ermsdorf                      2011 merged to form Vallée de l'Ernz\n 7 Eschweiler                    2015 merged into Wiltz              \n 8 Heiderscheid                  2011 merged into Esch-sur-Sûre      \n 9 Heinerscheid                  2011 merged into Clervaux           \n10 Hobscheid                     2018 merged to form Habscht         \n11 Hoscheid                      2011 merged to form Parc Hosingen   \n12 Hosingen                      2011 merged to form Parc Hosingen   \n13 Mompach                       2018 merged to form Rosport-Mompach \n14 Medernach                     2011 merged to form Vallée de l'Ernz\n15 Munshausen                    2011 merged into Clervaux           \n16 Neunhausen                    2011 merged into Esch-sur-Sûre      \n17 Rosport                       2018 merged to form Rosport-Mompach \n18 Septfontaines                 2018 merged to form Habscht         \n19 Tuntange                      2018 merged to form Helperknapp     \n20 Wellenstein                   2011 merged into Schengen           \n```\n:::\n:::\n\n\nAs you can see, since 2010 many communes have merged to form new ones. We can\nnow combine the list of current and former communes, as well as harmonise their\nnames:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncommunes <- unique(c(former_communes$name,\n                     current_communes$commune))\n# we need to rename some communes\n\n# Different spelling of these communes between wikipedia and the data\n\ncommunes[which(communes == \"Clemency\")] <- \"Clémency\"\ncommunes[which(communes == \"Redange\")] <- \"Redange-sur-Attert\"\ncommunes[which(communes == \"Erpeldange-sur-Sûre\")] <- \"Erpeldange\"\ncommunes[which(communes == \"Luxembourg City\")] <- \"Luxembourg\"\ncommunes[which(communes == \"Käerjeng\")] <- \"Kaerjeng\"\ncommunes[which(communes == \"Petange\")] <- \"Pétange\"\n```\n:::\n\n\nLet’s run our test again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetdiff(unique(commune_level_data$locality),\n        communes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncharacter(0)\n```\n:::\n:::\n\n\nGreat! When we compare the communes that are in our data with every commune that\nhas existed since 2010, we don’t have any commune that is unaccounted for. So\nare we done with cleaning the data? Yes, we can now start with analysing the\ndata. Take a look\n[here](https://raw.githubusercontent.com/b-rodrigues/rap4all/master/scripts/save_data.R)^[https://is.gd/7PhUjd]\nto see the finalised script. Also read some of the comments that I’ve added. This\nis a typical R script, and at first glance, one might wonder what is wrong with\nit. Actually, not much, but the problem if you leave this script as it is, is\nthat it is very likely that we will have problems rerunning it in the future. As\nit turns out, this script is not reproducible. But we will discuss this in much\nmore detail later on. For now, let’s analyse our cleaned data.\n\n## Analysing the data\n\nWe are now going to analyse the data. The first thing we are going to do is\ncompute a Laspeyeres price index. This price index allows us to make comparisons\nthrough time; for example, the index at year 2012 measures how much more\nexpensive (or cheaper) housing became relative to the base year (2010). However,\nsince we only have one 'good' (housing), this index becomes quite simple to\ncompute: it is nothing but the prices at year *t* divided by the prices in 2010\n(if we had a basket of goods, we would need to use the Laspeyeres index formula\nto compute the index at all periods).\n\nFor this section, I will perform a rather simple analysis. I will immediately\nshow you the R script: take a look at it\n[here](https://raw.githubusercontent.com/b-rodrigues/rap4all/master/scripts/analysis.R)^[https://is.gd/qCJEbi].\nFor the analysis I selected 5 communes and plotted the evolution of prices\ncompared to the national average.\n\nThis analysis might seem trivially simple, but it contains all the needed\ningredients to illustrate everything else that I’m going to teach you in this\nbook.\n\nMost analyses would stop here: after all, we have what we need; our goal was to\nget the plots for the 5 communes of Luxembourg, Esch-sur-Alzette, Mamer,\nSchengen (which gave its name to the [Schengen\nArea](https://en.wikipedia.org/wiki/Schengen_Area)^[https://en.wikipedia.org/wiki/Schengen_Area])\nand Wincrange. However, let’s ask ourselves the following important questions:\n\n- How easy would it be for someone else to rerun the analysis?\n- How easy would it be to update the analysis once new data gets published?\n- How easy would it be to reuse this code for other projects?\n- What guarantee do we have that if the scripts get run in 5 years, with the same input data, we get the same output?\n\nLet’s answer these questions one by one.\n\n## Your project is not done\n\n### How easy would it be for someone else to rerun the analysis?\n\nThe analysis is composed of two R scripts, one to prepare the data, and another\nto actually run the analysis proper. Performing the analysis might seem quite\neasy, because each script contains comments as to what is going on, and the code\nis not that complicated. However, we are missing any project-level documentation\nthat would provide clear instructions as to how to run the analysis. This might\nseem simple for us who wrote these scripts, but we are familiar with R, and this\nis still fresh in our brains. Should someone less familiar with R have to run\nthe script, there is no clue for them as to how they should do it. And of\ncourse, should the analysis be more complex (suppose it’s composed of dozens of\nscripts), this gets even worse. It might not even be easy for you to remember\nhow to run this in 5 months!\n\nAnd what about the required dependencies? Many packages were used in the\nanalysis. How should these get installed? Ideally, the same versions of the\npackages you used and the same version of R should get used by that person to\nrerun the analysis.\n\nAll of this still needs to be documented, but listing the packages that were\nused for an analysis and their versions takes quite some time. Thankfully, in\npart 2, we will learn about the `{renv}` package to deal with this in a couple\nlines of code.\n\n### How easy would it be to update the project?\n\nIf new data gets published, all the points discussed previously are still valid,\nplus you need to make sure that the updated data is still close enough to the\nprevious data such that it can pass through the data cleaning steps you wrote.\nYou should also make sure that the update did not introduce a mistake in past\ndata, or at least alert you if that is the case. Sometimes, when new years get\nadded, data for previous years also get corrected, so it would be nice to make\nsure that you know this. Also, in the specific case of our data, communes might\nget fused into a new one, or maybe even divided into smaller communes (even\nthough this has not happened in a long time, it is not entirely out of the\nquestion).\n\nIn summary, what is missing from the current project are enough tests to make\nsure that an update to the data can happen smoothly.\n\n### How easy would it be to reuse this code for another project?\n\nSaid plainly, not very easy. With code in this state you have no choice but to\ncopy and paste it into a new script and change it adequately. For re-usability,\nnothing beats structuring your code into functions and ideally you would even\npackage them. We are going to learn just that in future chapters of this book.\n\nBut sometimes you might not be interested in reusing code for another project:\nhowever, even if that’s the case, structuring your code into functions and\npackaging them makes it easy to reuse code even inside the same project. Look at\nthe last part of the `analysis.R` script: we copied and pasted the same code 5\ntimes and only slightly changed it. We are going to learn how not to repeat\nourselves by using functions and you will immediately see the benefits of\nwriting functions, even when simply reusing them inside the same project.\n\n### What guarantee do we have that the output is stable through time?\n\nNow this might seem weird: after all, if we start from the same dataset, does it\nmatter *when* we run the scripts? We should be getting the same result if we\nbuild the project today, in 5 months or in 5 years. Well, not necessarily. While\nit is true that R is quite stable, this cannot necessarily be said of the\npackages that we use. There is no guarantee that the authors of the packages\nwill not change the package’s functions to work differently, or take arguments\nin a different order, or even that the packages will all be available at all in\n5 years. And even if the packages are still available and function the same,\nbugs in the packages might get corrected which could alter the result. This\nmight seem like a non-problem; after all, if bugs get corrected, shouldn’t you\nbe happy to update your results as well? But this depends on what it is we’re\ntalking about. Sometimes it is necessary to reproduce results exactly as they\nwere, even if they were wrong, for example in the context of an audit.\n\nSo we also need a way to somehow snapshot and freeze the computational\nenvironment that was used to create the project originally.\n\n## Conclusion\n\nWe now have a basic analysis that has all we need to get started. In the coming\nchapters, we are going to learn about topics that will make it easy to write\ncode that is more robust, better documented and tested, and most importantly\neasy to rerun (and thus to reproduce the results). The first step will actually\nnot involve having to start rewriting our scripts though; next, we are going to\nlearn about Git, a tool that will make our life easier by versioning our code.\n",
    "supporting": [
      "project_start_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}